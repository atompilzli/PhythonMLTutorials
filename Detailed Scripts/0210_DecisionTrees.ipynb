{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forest\n",
    "\n",
    "## Decision Trees\n",
    "\n",
    "### Motivating Decision Trees\n",
    "\n",
    "Decision trees are extremely intuitive ways of classifying data: you simply ask a series of questions in order to close in on the class label. For example, if you are into value investing, you might think of the following scheme to decide whether you invest into a certain stock or not:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/0210_DecisionTreeTeX.png\" alt=\"DT_TeX\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(P/B = Price/Book ratio, P/E = Price/Earnings ratio, PEG = Price/Earnings to growth ratio, E = Equity, D = Debt).\n",
    "\n",
    "Sources tell me this scheme is not Warren Buffet's key to success - it is obviously an overly simplified illustration. But it serves well to demonstrate the idea behind decision trees in ML. The binary splits narrow down the options. The big question here though is of course what questions we ought to ask to derive the desired answer and in what sequence. This we will discuss in the next sections - first intuitively and then in more rigorous terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Decision Trees Illustrated\n",
    "\n",
    "To draw up a simple decision tree, we follow these two steps (James et al. (2013)): \n",
    "* Divide the predictor space (set of possible values for $X_1, X_2, \\ldots, X_p$) into $M$ distinct and non-overlapping regions $R_1, R_2, \\ldots, R_M$. \n",
    "* For every observation in $R_m$ we model the response as a constant. This constant is based on a majority vote among the observation in $R_m$. \n",
    "\n",
    "Let us illustrate this on a two dimensional data set with features $X_1$ and $X_2$. The color of the dots indicate the true class label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/0210_DTScatter.png\" alt=\"DT_Scatter\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without any split, we have but one region. A decision tree now splits the regions iteratively into predictor spaces $R_m$. This is shown below. The first figure on the left has two spaces, $R_1, R_2$, The second already has four ($R_1, R_2, R_3, R_4$) etc.. The background color indicates the label that the model would assign to a new data point in the respective area. Note that (as in the introductory decision tree figure) each branch can have a sperate number of splits. Some nodes are purer (get split more) than others. \n",
    "\n",
    "The `depth=n` argument in the figure title refers to the depth of the tree. Nodes that contain only a single class (color) are not further split. All others will be further split until a stopping criterion is reached, e.g. \n",
    "* all predictor spaces are pure (contain only one class), \n",
    "* all predictor spaces contain a limited number of data points,\n",
    "* we limit the number of splits through the `depth` argument etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/0210_DTSplits.png\" alt=\"DT_Splits\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Description\n",
    "#### Maximizing Information Gain\n",
    "\n",
    "How do we construct the regions $R_1, \\ldots, R_M$? Or put in other words: How do we decide on the splitting variables (i.e. $X_1, X_2, \\ldots, X_p$), split points and what topology (shape) the tree should have? To explain this we focus on the CART (classification and regression tree) approach of decision trees as implemented in Scikit-learn. \n",
    "\n",
    "In order to describe the mathematics let us assume we have a data set with $p$ inputs for each of the $N$ observations: $(x_i, y_i)$ for $i = 1, 2, \\ldots, N$, with $x_i = (x_{i1}, x_{i2}, \\ldots, x_{ip})$. \n",
    "\n",
    "Most libraries (including Scikit-learn) have implemented binary decision trees, meaning that each node is split into two child nodes. Hence for binary splits at node $m$ we take initial region $R_m$ and select $j$ (of feature $X_j$) and threshold $t_m$ such that the resulting two half-planes \n",
    "\n",
    "$$\\begin{equation}\n",
    "R_{\\text{left}}(R_m; j, t_m) = \\{(X, y) \\, | \\, X_j < t_m \\} \\qquad \\text{and} \\qquad R_{\\text{right}}(R_m; j, t_m) = \\{(X, y) \\, | \\, X_j \\geq t_m \\}\n",
    "\\end{equation}$$\n",
    "\n",
    "maximize the **information gain ($G$)** for any value of $j$ and $t_m$. \n",
    "\n",
    "Let us define $\\theta = (j, t_m)$ to simplify expressions. Given $R_m$ with $N_m$ being the total number of samples in $R_m$ and $n_{\\text{left}}$, $n_{\\text{right}}$ the number samples in the left ($R_{\\text{left}}$) and right ($R_{\\text{right}}$) child nodes, respectively, the information gain function $G$ is defined as \n",
    "\n",
    "$$\\begin{equation}\n",
    "G(R_m; \\theta) = H(R_m) - \\left( \\frac{n_{\\text{left}}}{N_m} H(R_{\\text{left}}) + \\frac{n_{\\text{right}}}{N_m} H(R_{\\text{right}})\\right)\n",
    "\\end{equation}$$\n",
    "\n",
    "Here, $H()$ is simply a measure of impurity which we will get to shortly. With that, the information gain $G$ is simply the difference between the impurity of the parent node and the sum of the child node impurities. The lower the impurity of the child nodes, the larger information gain we get (Raschka (2015)).\n",
    "\n",
    "Our optimization task can therefore be formulated as to find parameter $\\theta$ that maximizes the following expression at every node $m$:\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\theta^* = \\arg \\max_{\\theta} G(R_m; \\theta)\n",
    "\\end{equation}$$\n",
    "\n",
    "This is done recursively for every node until the stopping criteria is reached (max. depth, max. number of samples in region etc.; see above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impurity Measures for Classification Trees\n",
    "\n",
    "There are three common impurity measures (or splitting criteria), of which only the latter two are recommended for growing decision trees: Classification error rate ($H_E$), Gini index ($H_G$), cross-entropy ($H_H$). To discuss them, let us first define the proportion of class $k$ observations in node $m$ (Friedman et al. (2001)):\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\hat{p}_{mk} = \\frac{1}{N_m} \\sum_{x_i \\in R_m} I(y_i = k)\n",
    "\\end{equation}$$\n",
    "\n",
    "Earlier we mentioned that all observations in region $R_j$ are assigned to the same class following a majority vote. In more formal terms this means that observations in node $m$ are classified to class $k$ for which $k(m) = \\arg \\max_k \\hat{p}_{mk}$. \n",
    "\n",
    "Now, the impurity measures are defined as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "&\\text{Classification Error rate: } & H_E(R_m) &= 1 - \\max_k (p_{mk}) \\\\\n",
    "&\\text{Gini Index: } & H_G(R_m) &= \\sum_k p_{mk} (1 - p_{mk}) \\\\\n",
    "&\\text{Cross-Entropy: } & H_E(R_m) &= - \\sum_k p_{mk} \\, \\log(p_{mk})\n",
    "\\end{align}$$\n",
    "\n",
    "Note that for binary classification tasks (e.g. $y \\in \\{0, 1\\}$), $\\log$ in the cross-entropy is usually the [logarithm to the base 2.](https://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below figure graphs the three impurity measures with respect to $p$. As mentioned, the classification error rate should not be used as an impurity measure. In practice it is primarily used to prune a tree - a concept we will not discuss here. Cross-entropy is minimal if all samples at a node belong to the same class and maximal if we have a uniform distribution among the classes. Therefore, cross-entropy can be understood as a criterion that attempts to maximize the mutual information in the tree. The Gini-index on the other hand works towards minimizing the probability of misclassification. Similar to entropy it is maximal if classes are evenly distributed and minimal if the vast majority of samples belong to the same class. \n",
    "\n",
    "In practice both Gini index as well as cross-entropy usually produce similar results and thus it is not advisable to spend much time on evaluating trees using different impurity criteria (Raschka (2015))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/0210_ImpurityIndex.png\" alt=\"ImpurityIndex\" style=\"width: 1500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages of Decision Trees\n",
    "\n",
    "Decision trees have several advantages over the other classification approaches discussed so far:\n",
    "\n",
    "* Intuition is easy to explain\n",
    "* Trees can be displayed graphically and are easily interpreted even by a layperson\n",
    "* Trees handle quantitative as well as qualitative features; there's no need to scale the values\n",
    "* Some people argue that decision trees mirror human thinking very closely, moreso than other approaches.\n",
    "\n",
    "However, simple decision trees have also disadvantages, some of them are significant:\n",
    "* Instability of trees/high variance: Small changes in the data often result in very different series of splits\n",
    "* Decision trees tend to build complex decision boundaries, which often results in overfitting the data\n",
    "* Low level of predictive accuracy compared other classification approaches discussed in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees in Python\n",
    "\n",
    "To show how decision trees are applied in Python we once again rely on functions implemented in the Scikit-learn package. This time we will use the `Carseats` data set. It is again a data set that corresponds to James et al. (2013)'s book and contains information on child carseat sales at 400 different stores. `Sales` is the response variable with number of sold units in thousands. All other values are used as features. A detailed description of the data set can be found [here](https://cran.r-project.org/web/packages/ISLR/ISLR.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "      <th>CompPrice</th>\n",
       "      <th>Income</th>\n",
       "      <th>Advertising</th>\n",
       "      <th>Population</th>\n",
       "      <th>Price</th>\n",
       "      <th>ShelveLoc</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Urban</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.50</td>\n",
       "      <td>138</td>\n",
       "      <td>73</td>\n",
       "      <td>11</td>\n",
       "      <td>276</td>\n",
       "      <td>120</td>\n",
       "      <td>Bad</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.22</td>\n",
       "      <td>111</td>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "      <td>260</td>\n",
       "      <td>83</td>\n",
       "      <td>Good</td>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.06</td>\n",
       "      <td>113</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>269</td>\n",
       "      <td>80</td>\n",
       "      <td>Medium</td>\n",
       "      <td>59</td>\n",
       "      <td>12</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.40</td>\n",
       "      <td>117</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>466</td>\n",
       "      <td>97</td>\n",
       "      <td>Medium</td>\n",
       "      <td>55</td>\n",
       "      <td>14</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.15</td>\n",
       "      <td>141</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>340</td>\n",
       "      <td>128</td>\n",
       "      <td>Bad</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sales  CompPrice  Income  Advertising  Population  Price ShelveLoc  Age  \\\n",
       "0   9.50        138      73           11         276    120       Bad   42   \n",
       "1  11.22        111      48           16         260     83      Good   65   \n",
       "2  10.06        113      35           10         269     80    Medium   59   \n",
       "3   7.40        117     100            4         466     97    Medium   55   \n",
       "4   4.15        141      64            3         340    128       Bad   38   \n",
       "\n",
       "   Education Urban   US  \n",
       "0         17   Yes  Yes  \n",
       "1         10   Yes  Yes  \n",
       "2         12   Yes  Yes  \n",
       "3         14   Yes  Yes  \n",
       "4         13   Yes   No  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('Data/Carseats.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume you are a financial consultant and work on a market study that discusses appropriate sales channels for child car seats. You know that your client's strategy is to grow the business. Furthermore you learned that to run sales at break even at a sales location the company needs to sell approximately 4'000 units. What feature drives sales and in which stores would you advise your client to offer their product? Here a decision tree helps very much in visualizing the sales driver. \n",
    "\n",
    "Before we implement the model we prepare the data. Notice that decision trees are invariant to scaling meaning we could, but don't have to scale values. This is true for both quantitative as well as categorical variables. What we need to do though, is transforming categorical values `ShelveLoc, Urban` and `US` into numeric values. We will use pandas `map` method to (a) show an alternative to `pd.factorize` introduced in previous chapters and (b) ensure a mapping that does not confuse (`pd.factorize()` maps a column's first entry to value 0, second to 1 etc. This would mean that `pd.factorize()` would label `Yes` in columns `Urban` or `US` as 0. Yet `Yes` is predominantly represented by a 1. Therefore, with the `map` function we preclude any confusion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sales  CompPrice  Income  Advertising  Population  Price  ShelveLoc  Age  \\\n",
      "0   9.50        138      73           11         276    120          0   42   \n",
      "1  11.22        111      48           16         260     83          2   65   \n",
      "2  10.06        113      35           10         269     80          1   59   \n",
      "\n",
      "   Education  Urban  US  BreakEven  \n",
      "0         17      1   1          1  \n",
      "1         10      1   1          1  \n",
      "2         12      1   1          1  \n"
     ]
    }
   ],
   "source": [
    "# Create 'BreakEven' column with 1 if Sales >= 4k, else 0\n",
    "df['BreakEven'] = df.Sales.map(lambda x: 1 if x>=4 else 0)\n",
    "\n",
    "# Replace category names with numbers\n",
    "df.ShelveLoc = df.ShelveLoc.map({'Bad':0, 'Medium': 1, 'Good': 2})\n",
    "df.Urban     = df.Urban.map({'No':0, 'Yes':1})\n",
    "df.US        = df.US.map({'No':0, 'Yes':1})\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assign features & response to X and y, respectively\n",
    "X = df.drop(['Sales', 'BreakEven'], axis=1)\n",
    "y = df.BreakEven\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are in a position to run the `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=4)\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a decision tree is as simple as that. From the above output we see that per default the function will use the Gini index as criterion. The only argument we set is the maximal depth. Alternatively we could define the maximum tree nodes, the minimum number of samples required to split an internal node or the minimum number of samples required to be at a leaf node. There are more options and it is best to check the [documentation page](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) to get a thorough understanding of the available options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with all other Scikit-learn functions, we can again call the usual performance metrics. The interpretation is as discussed in chapter 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True proportion of sales >= 4k:  0.91\n",
      "Train score:  0.94375\n",
      "Test score:  0.8875\n",
      "-------------------------------------\n",
      "Confusion matrix: \n",
      " [[ 0  4]\n",
      " [ 5 71]]\n",
      "Predicted sales >=4k   No   Yes\n",
      "True sales >=4k                \n",
      "No                    NaN   4.0\n",
      "Yes                   5.0  71.0\n"
     ]
    }
   ],
   "source": [
    "# Load metrics sublibrary\n",
    "from sklearn import metrics\n",
    "\n",
    "# Print performance metrics\n",
    "print('True proportion of sales >= 4k: ', y.sum() / y.shape[0])\n",
    "print('Train score: ', tree.score(X_train, y_train))\n",
    "print('Test score: ', tree.score(X_test, y_test))\n",
    "print(37*'-')\n",
    "\n",
    "# Confusion matrix\n",
    "y_pred = tree.predict(X_test)\n",
    "print('Confusion matrix: \\n', \n",
    "      metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Manual confusion matrix as pandas DataFrame\n",
    "confm = pd.DataFrame({'Predicted sales >=4k': y_pred,\n",
    "                      'True sales >=4k': y_test})\n",
    "confm.replace(to_replace={0:'No', 1:'Yes'}, inplace=True)\n",
    "print(confm.groupby(['True sales >=4k','Predicted sales >=4k']).size().unstack('Predicted sales >=4k'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Decision Trees\n",
    "\n",
    "Unfortunately, neither `matplotlib` nor `sklearn` have a plotting function integrated to visualize decision trees. However, with the help of additional libraries/packages, Python is able to create a decision tree figure detailing each decision step. To plot these decision trees, we have to rely on third party software. Getting things to run as expected has (at least in my case) proven to be a rather cumbersome experience. To spare you this experience, follow these steps (which at the time of this writing in December 2017 have solved the problem):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download and install Graphviz from [the web](http://www.graphviz.org/download/). Mac users select the `.pkg` file corresponding to your system version (Snow Leopard, Lion etc.). Windows users select the `.msi` file. Run the execution file and follow the instructions.\n",
    "* Type in `!pip list` into an input field in a Jupyter notebook. This will list all installed Python packages (and its versions). Check if you have a package called `graphviz`. If yes, proceed with next step. If not, type `pip install graphviz` into a new command line. This should install the needed Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alabaster (0.7.10)\n",
      "anaconda-client (1.6.5)\n",
      "anaconda-navigator (1.6.8)\n",
      "anaconda-project (0.8.0)\n",
      "asn1crypto (0.22.0)\n",
      "astroid (1.5.3)\n",
      "astropy (2.0.2)\n",
      "babel (2.5.0)\n",
      "backports.shutil-get-terminal-size (1.0.0)\n",
      "basemap (1.1.0)\n",
      "beautifulsoup4 (4.6.0)\n",
      "bitarray (0.8.1)\n",
      "bkcharts (0.2)\n",
      "blaze (0.11.3)\n",
      "bleach (2.0.0)\n",
      "bokeh (0.12.7)\n",
      "boto (2.48.0)\n",
      "Bottleneck (1.2.1)\n",
      "CacheControl (0.12.3)\n",
      "certifi (2018.1.18)\n",
      "cffi (1.10.0)\n",
      "chardet (3.0.4)\n",
      "click (6.7)\n",
      "cloudpickle (0.4.0)\n",
      "clyent (1.2.2)\n",
      "colorama (0.3.9)\n",
      "comtypes (1.1.2)\n",
      "conda (4.4.8)\n",
      "conda-build (3.0.22)\n",
      "conda-verify (2.0.0)\n",
      "contextlib2 (0.5.5)\n",
      "cryptography (2.0.3)\n",
      "cycler (0.10.0)\n",
      "Cython (0.26.1)\n",
      "cytoolz (0.8.2)\n",
      "dask (0.15.2)\n",
      "datashape (0.5.4)\n",
      "decorator (4.1.2)\n",
      "distlib (0.2.5)\n",
      "distributed (1.18.3)\n",
      "docutils (0.14)\n",
      "entrypoints (0.2.3)\n",
      "et-xmlfile (1.0.1)\n",
      "fastcache (1.0.2)\n",
      "filelock (2.0.12)\n",
      "Flask (0.12.2)\n",
      "Flask-Cors (3.0.3)\n",
      "gevent (1.2.2)\n",
      "glob2 (0.5)\n",
      "graphviz (0.8.1)\n",
      "greenlet (0.4.12)\n",
      "h5py (2.7.0)\n",
      "heapdict (1.0.0)\n",
      "html5lib (0.999999999)\n",
      "idna (2.6)\n",
      "imageio (2.2.0)\n",
      "imagesize (0.7.1)\n",
      "ipykernel (4.6.1)\n",
      "ipython (6.1.0)\n",
      "ipython-genutils (0.2.0)\n",
      "ipywidgets (7.0.0)\n",
      "isort (4.2.15)\n",
      "itsdangerous (0.24)\n",
      "jdcal (1.3)\n",
      "jedi (0.10.2)\n",
      "Jinja2 (2.9.6)\n",
      "jsonschema (2.6.0)\n",
      "jupyter-client (5.1.0)\n",
      "jupyter-console (5.2.0)\n",
      "jupyter-core (4.3.0)\n",
      "jupyterlab (0.27.0)\n",
      "jupyterlab-launcher (0.4.0)\n",
      "lazy-object-proxy (1.3.1)\n",
      "llvmlite (0.20.0)\n",
      "locket (0.2.0)\n",
      "lockfile (0.12.2)\n",
      "lxml (3.8.0)\n",
      "MarkupSafe (1.0)\n",
      "matplotlib (2.0.2)\n",
      "mccabe (0.6.1)\n",
      "menuinst (1.4.8)\n",
      "mistune (0.7.4)\n",
      "mpmath (0.19)\n",
      "msgpack-python (0.4.8)\n",
      "multipledispatch (0.4.9)\n",
      "navigator-updater (0.1.0)\n",
      "nbconvert (5.3.1)\n",
      "nbformat (4.4.0)\n",
      "netCDF4 (1.3.1)\n",
      "networkx (1.11)\n",
      "nltk (3.2.4)\n",
      "nose (1.3.7)\n",
      "notebook (5.0.0)\n",
      "numba (0.35.0+10.g143f70e)\n",
      "numexpr (2.6.2)\n",
      "numpy (1.13.1)\n",
      "numpydoc (0.7.0)\n",
      "odo (0.5.1)\n",
      "olefile (0.44)\n",
      "openpyxl (2.4.8)\n",
      "packaging (16.8)\n",
      "pandas (0.20.3)\n",
      "pandas-datareader (0.5.0)\n",
      "pandocfilters (1.4.2)\n",
      "partd (0.3.8)\n",
      "path.py (10.3.1)\n",
      "pathlib2 (2.3.0)\n",
      "patsy (0.4.1)\n",
      "pep8 (1.7.0)\n",
      "pickleshare (0.7.4)\n",
      "Pillow (4.2.1)\n",
      "pip (9.0.1)\n",
      "pkginfo (1.4.1)\n",
      "ply (3.10)\n",
      "progress (1.3)\n",
      "prompt-toolkit (1.0.15)\n",
      "psutil (5.2.2)\n",
      "py (1.4.34)\n",
      "pycodestyle (2.3.1)\n",
      "pycosat (0.6.3)\n",
      "pycparser (2.18)\n",
      "pycrypto (2.6.1)\n",
      "pycurl (7.43.0)\n",
      "pydotplus (2.0.2)\n",
      "pyflakes (1.5.0)\n",
      "Pygments (2.2.0)\n",
      "pylint (1.7.2)\n",
      "pyodbc (4.0.17)\n",
      "pyOpenSSL (17.2.0)\n",
      "pyparsing (2.2.0)\n",
      "pyproj (1.9.5.1)\n",
      "pyshp (1.2.12)\n",
      "PySocks (1.6.7)\n",
      "pytest (3.2.1)\n",
      "python-dateutil (2.6.1)\n",
      "pytz (2017.2)\n",
      "PyWavelets (0.5.2)\n",
      "pywin32 (221)\n",
      "PyYAML (3.12)\n",
      "pyzmq (16.0.2)\n",
      "QtAwesome (0.4.4)\n",
      "qtconsole (4.3.1)\n",
      "QtPy (1.3.1)\n",
      "requests (2.18.4)\n",
      "requests-file (1.4.1)\n",
      "requests-ftp (0.3.1)\n",
      "rope (0.10.5)\n",
      "ruamel-yaml (0.11.14)\n",
      "scikit-image (0.13.0)\n",
      "scikit-learn (0.19.0)\n",
      "scipy (0.19.1)\n",
      "seaborn (0.8)\n",
      "setuptools (36.5.0.post20170921)\n",
      "Shapely (1.6.2)\n",
      "simplegeneric (0.8.1)\n",
      "singledispatch (3.4.0.3)\n",
      "six (1.10.0)\n",
      "snowballstemmer (1.2.1)\n",
      "sortedcollections (0.5.3)\n",
      "sortedcontainers (1.5.7)\n",
      "Sphinx (1.6.3)\n",
      "sphinxcontrib-websupport (1.0.1)\n",
      "spyder (3.2.3)\n",
      "SQLAlchemy (1.1.13)\n",
      "statsmodels (0.8.0)\n",
      "sympy (1.1.1)\n",
      "tables (3.4.2)\n",
      "tblib (1.3.2)\n",
      "testpath (0.3.1)\n",
      "toolz (0.8.2)\n",
      "tornado (4.5.2)\n",
      "traitlets (4.3.2)\n",
      "typing (3.6.2)\n",
      "unicodecsv (0.14.1)\n",
      "urllib3 (1.22)\n",
      "wcwidth (0.1.7)\n",
      "webencodings (0.5.1)\n",
      "Werkzeug (0.12.2)\n",
      "wheel (0.29.0)\n",
      "widgetsnbextension (3.0.2)\n",
      "win-inet-pton (1.0.1)\n",
      "win-unicode-console (0.5)\n",
      "wincertstore (0.2)\n",
      "wrapt (1.10.11)\n",
      "xlrd (1.1.0)\n",
      "XlsxWriter (0.9.8)\n",
      "xlwings (0.11.4)\n",
      "xlwt (1.3.0)\n",
      "zict (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\n",
      "You are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, we need to make sure that Graphviz' `.exe` file was added to your system `PATH`. [This is an environmental variable that tells the computer which directories to search for executable files (i.e., ready-to-run programs) in response to commands issued by a user](http://www.linfo.org/path_env_var.html). How do you know if Graphviz is on your `PATH`? Run below code to find out. (Alternatively you might open a shell window and on Windows call `$env:path.split(\";\")` or on a Mac `echo $PATH$`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\ImageMagick-7.0.5-Q16;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files\\MiKTeX 2.9\\miktex\\bin\\x64\\;C:\\Program Files\\Git\\cmd;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Anaconda;C:\\Anaconda\\Library\\mingw-w64\\bin;C:\\Anaconda\\Library\\usr\\bin;C:\\Anaconda\\Library\\bin;C:\\Anaconda\\Scripts;C:\\Users\\Ben Zimmermann\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\Ben Zimmermann\\AppData\\Local\\GitHubDesktop\\bin;C:\\Program Files (x86)\\Graphviz2.38\\bin;;%USERPROFILE%\\AppData\\Local\\Microsoft\\WindowsApps'\n"
     ]
    }
   ],
   "source": [
    "# On a Windows PC (--> remove # to have output)\n",
    "%echo %PATH%'\n",
    "\n",
    "# On a Mac\n",
    "#%echo $PATH$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You see that in my case, Graphviz2.38 is the last entry on the list. In case it is missing on your machine, add graphviz to the `PATH`, by following the steps described [in this discussion on StackOverflow](https://stackoverflow.com/questions/18438997/why-is-pydot-unable-to-find-graphvizs-executables-in-windows-8). But be careful: This might cause some problems in the background with the Spyder IDE ([see discussion here](https://github.com/ContinuumIO/anaconda-issues/issues/1666))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having sorted this Graphviz installation out, we are now able to proceed with plotting the decision tree. Of the function's available input arguments (as in below's code snippet), `filled=True, rounded=True, class_names=['Loss', 'BrEven']` and `feature_names=X.columns.values` are optional but help in making the image visually more appealing by adding color, rounded box edges, showing the class labels counts at each node, and displaying the feature name according to the majority vote at the respective node. The plot below helps understand this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"1158pt\" height=\"552pt\"\r\n",
       " viewBox=\"0.00 0.00 1158.00 552.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 548)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-548 1154,-548 1154,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.890196\" stroke=\"black\" d=\"M666.5,-544C666.5,-544 562.5,-544 562.5,-544 556.5,-544 550.5,-538 550.5,-532 550.5,-532 550.5,-473 550.5,-473 550.5,-467 556.5,-461 562.5,-461 562.5,-461 666.5,-461 666.5,-461 672.5,-461 678.5,-467 678.5,-473 678.5,-473 678.5,-532 678.5,-532 678.5,-538 672.5,-544 666.5,-544\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"614.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">ShelveLoc &lt;= 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"614.5\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.18</text>\r\n",
       "<text text-anchor=\"middle\" x=\"614.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 320</text>\r\n",
       "<text text-anchor=\"middle\" x=\"614.5\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [32, 288]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"614.5\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = BrEven</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.607843\" stroke=\"black\" d=\"M565,-425C565,-425 476,-425 476,-425 470,-425 464,-419 464,-413 464,-413 464,-354 464,-354 464,-348 470,-342 476,-342 476,-342 565,-342 565,-342 571,-342 577,-348 577,-354 577,-354 577,-413 577,-413 577,-419 571,-425 565,-425\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"520.5\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Price &lt;= 149.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"520.5\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.405</text>\r\n",
       "<text text-anchor=\"middle\" x=\"520.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 78</text>\r\n",
       "<text text-anchor=\"middle\" x=\"520.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [22, 56]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"520.5\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = BrEven</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M581.887,-460.907C574.669,-451.923 566.95,-442.315 559.506,-433.05\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"562.047,-430.624 553.055,-425.021 556.59,-435.009 562.047,-430.624\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"550.346\" y=\"-446.174\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 12 -->\r\n",
       "<g id=\"node13\" class=\"node\"><title>12</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.956863\" stroke=\"black\" d=\"M757.5,-425C757.5,-425 659.5,-425 659.5,-425 653.5,-425 647.5,-419 647.5,-413 647.5,-413 647.5,-354 647.5,-354 647.5,-348 653.5,-342 659.5,-342 659.5,-342 757.5,-342 757.5,-342 763.5,-342 769.5,-348 769.5,-354 769.5,-354 769.5,-413 769.5,-413 769.5,-419 763.5,-425 757.5,-425\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"708.5\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Price &lt;= 157.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"708.5\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.079</text>\r\n",
       "<text text-anchor=\"middle\" x=\"708.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 242</text>\r\n",
       "<text text-anchor=\"middle\" x=\"708.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [10, 232]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"708.5\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = BrEven</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;12 -->\r\n",
       "<g id=\"edge12\" class=\"edge\"><title>0&#45;&gt;12</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M647.113,-460.907C654.331,-451.923 662.05,-442.315 669.494,-433.05\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"672.41,-435.009 675.945,-425.021 666.953,-430.624 672.41,-435.009\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"678.654\" y=\"-446.174\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.690196\" stroke=\"black\" d=\"M348,-306C348,-306 259,-306 259,-306 253,-306 247,-300 247,-294 247,-294 247,-235 247,-235 247,-229 253,-223 259,-223 259,-223 348,-223 348,-223 354,-223 360,-229 360,-235 360,-235 360,-294 360,-294 360,-300 354,-306 348,-306\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"303.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Age &lt;= 61.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"303.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.361</text>\r\n",
       "<text text-anchor=\"middle\" x=\"303.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 72</text>\r\n",
       "<text text-anchor=\"middle\" x=\"303.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [17, 55]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"303.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = BrEven</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M463.769,-351.912C434.863,-336.327 399.554,-317.289 369.442,-301.054\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"370.739,-297.777 360.276,-296.112 367.417,-303.939 370.739,-297.777\"/>\r\n",
       "</g>\r\n",
       "<!-- 9 -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>9</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.800000\" stroke=\"black\" d=\"M583,-306C583,-306 458,-306 458,-306 452,-306 446,-300 446,-294 446,-294 446,-235 446,-235 446,-229 452,-223 458,-223 458,-223 583,-223 583,-223 589,-223 595,-229 595,-235 595,-235 595,-294 595,-294 595,-300 589,-306 583,-306\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"520.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">CompPrice &lt;= 148.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"520.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.278</text>\r\n",
       "<text text-anchor=\"middle\" x=\"520.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\r\n",
       "<text text-anchor=\"middle\" x=\"520.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [5, 1]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"520.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Loss</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;9 -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>1&#45;&gt;9</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M520.5,-341.907C520.5,-333.649 520.5,-324.864 520.5,-316.302\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"524,-316.021 520.5,-306.021 517,-316.021 524,-316.021\"/>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.874510\" stroke=\"black\" d=\"M217,-187C217,-187 124,-187 124,-187 118,-187 112,-181 112,-175 112,-175 112,-116 112,-116 112,-110 118,-104 124,-104 124,-104 217,-104 217,-104 223,-104 229,-110 229,-116 229,-116 229,-175 229,-175 229,-181 223,-187 217,-187\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Income &lt;= 27.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.198</text>\r\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 45</text>\r\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [5, 40]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = BrEven</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M257.356,-222.907C246.628,-213.469 235.116,-203.343 224.097,-193.649\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"226.382,-190.998 216.562,-187.021 221.759,-196.254 226.382,-190.998\"/>\r\n",
       "</g>\r\n",
       "<!-- 6 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.200000\" stroke=\"black\" d=\"M348,-187C348,-187 259,-187 259,-187 253,-187 247,-181 247,-175 247,-175 247,-116 247,-116 247,-110 253,-104 259,-104 259,-104 348,-104 348,-104 354,-104 360,-110 360,-116 360,-116 360,-175 360,-175 360,-181 354,-187 348,-187\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"303.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Price &lt;= 122.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"303.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.494</text>\r\n",
       "<text text-anchor=\"middle\" x=\"303.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 27</text>\r\n",
       "<text text-anchor=\"middle\" x=\"303.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [12, 15]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"303.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = BrEven</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;6 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>2&#45;&gt;6</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M303.5,-222.907C303.5,-214.649 303.5,-205.864 303.5,-197.302\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"307,-197.021 303.5,-187.021 300,-197.021 307,-197.021\"/>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M85,-68C85,-68 12,-68 12,-68 6,-68 0,-62 0,-56 0,-56 0,-12 0,-12 0,-6 6,-0 12,-0 12,-0 85,-0 85,-0 91,-0 97,-6 97,-12 97,-12 97,-56 97,-56 97,-62 91,-68 85,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"48.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"48.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\r\n",
       "<text text-anchor=\"middle\" x=\"48.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1, 0]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"48.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Loss</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>3&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M125.072,-103.726C114.605,-94.3318 103.482,-84.349 93.0532,-74.9883\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"95.3808,-72.3744 85.6009,-68.2996 90.7051,-77.5839 95.3808,-72.3744\"/>\r\n",
       "</g>\r\n",
       "<!-- 5 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.901961\" stroke=\"black\" d=\"M215.5,-68C215.5,-68 127.5,-68 127.5,-68 121.5,-68 115.5,-62 115.5,-56 115.5,-56 115.5,-12 115.5,-12 115.5,-6 121.5,-0 127.5,-0 127.5,-0 215.5,-0 215.5,-0 221.5,-0 227.5,-6 227.5,-12 227.5,-12 227.5,-56 227.5,-56 227.5,-62 221.5,-68 215.5,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"171.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.165</text>\r\n",
       "<text text-anchor=\"middle\" x=\"171.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 44</text>\r\n",
       "<text text-anchor=\"middle\" x=\"171.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [4, 40]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"171.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = BrEven</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;5 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>3&#45;&gt;5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M170.872,-103.726C170.947,-95.5175 171.026,-86.8595 171.102,-78.56\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"174.604,-78.3312 171.196,-68.2996 167.605,-78.2672 174.604,-78.3312\"/>\r\n",
       "</g>\r\n",
       "<!-- 7 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>7</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.498039\" stroke=\"black\" d=\"M346.5,-68C346.5,-68 258.5,-68 258.5,-68 252.5,-68 246.5,-62 246.5,-56 246.5,-56 246.5,-12 246.5,-12 246.5,-6 252.5,-0 258.5,-0 258.5,-0 346.5,-0 346.5,-0 352.5,-0 358.5,-6 358.5,-12 358.5,-12 358.5,-56 358.5,-56 358.5,-62 352.5,-68 346.5,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"302.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\r\n",
       "<text text-anchor=\"middle\" x=\"302.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 21</text>\r\n",
       "<text text-anchor=\"middle\" x=\"302.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [7, 14]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"302.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = BrEven</text>\r\n",
       "</g>\r\n",
       "<!-- 6&#45;&gt;7 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>6&#45;&gt;7</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M303.128,-103.726C303.053,-95.5175 302.974,-86.8595 302.898,-78.56\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"306.395,-78.2672 302.804,-68.2996 299.396,-78.3312 306.395,-78.2672\"/>\r\n",
       "</g>\r\n",
       "<!-- 8 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>8</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.800000\" stroke=\"black\" d=\"M462,-68C462,-68 389,-68 389,-68 383,-68 377,-62 377,-56 377,-56 377,-12 377,-12 377,-6 383,-0 389,-0 389,-0 462,-0 462,-0 468,-0 474,-6 474,-12 474,-12 474,-56 474,-56 474,-62 468,-68 462,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"425.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.278</text>\r\n",
       "<text text-anchor=\"middle\" x=\"425.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\r\n",
       "<text text-anchor=\"middle\" x=\"425.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [5, 1]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"425.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Loss</text>\r\n",
       "</g>\r\n",
       "<!-- 6&#45;&gt;8 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>6&#45;&gt;8</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M348.928,-103.726C359.395,-94.3318 370.518,-84.349 380.947,-74.9883\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"383.295,-77.5839 388.399,-68.2996 378.619,-72.3744 383.295,-77.5839\"/>\r\n",
       "</g>\r\n",
       "<!-- 10 -->\r\n",
       "<g id=\"node11\" class=\"node\"><title>10</title>\r\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M463,-179.5C463,-179.5 390,-179.5 390,-179.5 384,-179.5 378,-173.5 378,-167.5 378,-167.5 378,-123.5 378,-123.5 378,-117.5 384,-111.5 390,-111.5 390,-111.5 463,-111.5 463,-111.5 469,-111.5 475,-117.5 475,-123.5 475,-123.5 475,-167.5 475,-167.5 475,-173.5 469,-179.5 463,-179.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"426.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"426.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"426.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [5, 0]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"426.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Loss</text>\r\n",
       "</g>\r\n",
       "<!-- 9&#45;&gt;10 -->\r\n",
       "<g id=\"edge10\" class=\"edge\"><title>9&#45;&gt;10</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M487.887,-222.907C478.756,-211.542 468.823,-199.178 459.66,-187.774\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"462.139,-185.271 453.147,-179.667 456.682,-189.655 462.139,-185.271\"/>\r\n",
       "</g>\r\n",
       "<!-- 11 -->\r\n",
       "<g id=\"node12\" class=\"node\"><title>11</title>\r\n",
       "<path fill=\"#399de5\" stroke=\"black\" d=\"M593.5,-179.5C593.5,-179.5 505.5,-179.5 505.5,-179.5 499.5,-179.5 493.5,-173.5 493.5,-167.5 493.5,-167.5 493.5,-123.5 493.5,-123.5 493.5,-117.5 499.5,-111.5 505.5,-111.5 505.5,-111.5 593.5,-111.5 593.5,-111.5 599.5,-111.5 605.5,-117.5 605.5,-123.5 605.5,-123.5 605.5,-167.5 605.5,-167.5 605.5,-173.5 599.5,-179.5 593.5,-179.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"549.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"549.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\r\n",
       "<text text-anchor=\"middle\" x=\"549.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"549.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = BrEven</text>\r\n",
       "</g>\r\n",
       "<!-- 9&#45;&gt;11 -->\r\n",
       "<g id=\"edge11\" class=\"edge\"><title>9&#45;&gt;11</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M530.561,-222.907C533.242,-212.094 536.146,-200.376 538.856,-189.441\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"542.27,-190.215 541.279,-179.667 535.476,-188.531 542.27,-190.215\"/>\r\n",
       "</g>\r\n",
       "<!-- 13 -->\r\n",
       "<g id=\"node14\" class=\"node\"><title>13</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.972549\" stroke=\"black\" d=\"M753,-306C753,-306 664,-306 664,-306 658,-306 652,-300 652,-294 652,-294 652,-235 652,-235 652,-229 658,-223 664,-223 664,-223 753,-223 753,-223 759,-223 765,-229 765,-235 765,-235 765,-294 765,-294 765,-300 759,-306 753,-306\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"708.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Price &lt;= 124.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"708.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.05</text>\r\n",
       "<text text-anchor=\"middle\" x=\"708.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 232</text>\r\n",
       "<text text-anchor=\"middle\" x=\"708.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [6, 226]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"708.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = BrEven</text>\r\n",
       "</g>\r\n",
       "<!-- 12&#45;&gt;13 -->\r\n",
       "<g id=\"edge13\" class=\"edge\"><title>12&#45;&gt;13</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M708.5,-341.907C708.5,-333.649 708.5,-324.864 708.5,-316.302\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"712,-316.021 708.5,-306.021 705,-316.021 712,-316.021\"/>\r\n",
       "</g>\r\n",
       "<!-- 18 -->\r\n",
       "<g id=\"node19\" class=\"node\"><title>18</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.333333\" stroke=\"black\" d=\"M1022.5,-306C1022.5,-306 934.5,-306 934.5,-306 928.5,-306 922.5,-300 922.5,-294 922.5,-294 922.5,-235 922.5,-235 922.5,-229 928.5,-223 934.5,-223 934.5,-223 1022.5,-223 1022.5,-223 1028.5,-223 1034.5,-229 1034.5,-235 1034.5,-235 1034.5,-294 1034.5,-294 1034.5,-300 1028.5,-306 1022.5,-306\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"978.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Age &lt;= 60.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"978.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.48</text>\r\n",
       "<text text-anchor=\"middle\" x=\"978.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 10</text>\r\n",
       "<text text-anchor=\"middle\" x=\"978.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [4, 6]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"978.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = BrEven</text>\r\n",
       "</g>\r\n",
       "<!-- 12&#45;&gt;18 -->\r\n",
       "<g id=\"edge18\" class=\"edge\"><title>12&#45;&gt;18</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M769.767,-355.951C812.383,-337.484 869.14,-312.889 912.711,-294.008\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"914.295,-297.137 922.079,-289.949 911.512,-290.714 914.295,-297.137\"/>\r\n",
       "</g>\r\n",
       "<!-- 14 -->\r\n",
       "<g id=\"node15\" class=\"node\"><title>14</title>\r\n",
       "<path fill=\"#399de5\" stroke=\"black\" d=\"M725,-179.5C725,-179.5 636,-179.5 636,-179.5 630,-179.5 624,-173.5 624,-167.5 624,-167.5 624,-123.5 624,-123.5 624,-117.5 630,-111.5 636,-111.5 636,-111.5 725,-111.5 725,-111.5 731,-111.5 737,-117.5 737,-123.5 737,-123.5 737,-167.5 737,-167.5 737,-173.5 731,-179.5 725,-179.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"680.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"680.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 151</text>\r\n",
       "<text text-anchor=\"middle\" x=\"680.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 151]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"680.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = BrEven</text>\r\n",
       "</g>\r\n",
       "<!-- 13&#45;&gt;14 -->\r\n",
       "<g id=\"edge14\" class=\"edge\"><title>13&#45;&gt;14</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M698.786,-222.907C696.198,-212.094 693.393,-200.376 690.777,-189.441\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"694.169,-188.578 688.437,-179.667 687.361,-190.207 694.169,-188.578\"/>\r\n",
       "</g>\r\n",
       "<!-- 15 -->\r\n",
       "<g id=\"node16\" class=\"node\"><title>15</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.921569\" stroke=\"black\" d=\"M892,-187C892,-187 767,-187 767,-187 761,-187 755,-181 755,-175 755,-175 755,-116 755,-116 755,-110 761,-104 767,-104 767,-104 892,-104 892,-104 898,-104 904,-110 904,-116 904,-116 904,-175 904,-175 904,-181 898,-187 892,-187\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"829.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">CompPrice &lt;= 112.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"829.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.137</text>\r\n",
       "<text text-anchor=\"middle\" x=\"829.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 81</text>\r\n",
       "<text text-anchor=\"middle\" x=\"829.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [6, 75]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"829.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = BrEven</text>\r\n",
       "</g>\r\n",
       "<!-- 13&#45;&gt;15 -->\r\n",
       "<g id=\"edge15\" class=\"edge\"><title>13&#45;&gt;15</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M750.48,-222.907C760.053,-213.651 770.311,-203.732 780.16,-194.209\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"782.838,-196.488 787.594,-187.021 777.972,-191.456 782.838,-196.488\"/>\r\n",
       "</g>\r\n",
       "<!-- 16 -->\r\n",
       "<g id=\"node17\" class=\"node\"><title>16</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.250980\" stroke=\"black\" d=\"M808.5,-68C808.5,-68 720.5,-68 720.5,-68 714.5,-68 708.5,-62 708.5,-56 708.5,-56 708.5,-12 708.5,-12 708.5,-6 714.5,-0 720.5,-0 720.5,-0 808.5,-0 808.5,-0 814.5,-0 820.5,-6 820.5,-12 820.5,-12 820.5,-56 820.5,-56 820.5,-62 814.5,-68 808.5,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"764.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.49</text>\r\n",
       "<text text-anchor=\"middle\" x=\"764.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 7</text>\r\n",
       "<text text-anchor=\"middle\" x=\"764.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [3, 4]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"764.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = BrEven</text>\r\n",
       "</g>\r\n",
       "<!-- 15&#45;&gt;16 -->\r\n",
       "<g id=\"edge16\" class=\"edge\"><title>15&#45;&gt;16</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M805.296,-103.726C800.099,-94.9703 794.598,-85.7032 789.375,-76.9051\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"792.381,-75.1121 784.267,-68.2996 786.362,-78.6853 792.381,-75.1121\"/>\r\n",
       "</g>\r\n",
       "<!-- 17 -->\r\n",
       "<g id=\"node18\" class=\"node\"><title>17</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.956863\" stroke=\"black\" d=\"M938.5,-68C938.5,-68 850.5,-68 850.5,-68 844.5,-68 838.5,-62 838.5,-56 838.5,-56 838.5,-12 838.5,-12 838.5,-6 844.5,-0 850.5,-0 850.5,-0 938.5,-0 938.5,-0 944.5,-0 950.5,-6 950.5,-12 950.5,-12 950.5,-56 950.5,-56 950.5,-62 944.5,-68 938.5,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"894.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.078</text>\r\n",
       "<text text-anchor=\"middle\" x=\"894.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 74</text>\r\n",
       "<text text-anchor=\"middle\" x=\"894.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [3, 71]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"894.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = BrEven</text>\r\n",
       "</g>\r\n",
       "<!-- 15&#45;&gt;17 -->\r\n",
       "<g id=\"edge17\" class=\"edge\"><title>15&#45;&gt;17</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M853.704,-103.726C858.901,-94.9703 864.402,-85.7032 869.625,-76.9051\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"872.638,-78.6853 874.733,-68.2996 866.619,-75.1121 872.638,-78.6853\"/>\r\n",
       "</g>\r\n",
       "<!-- 19 -->\r\n",
       "<g id=\"node20\" class=\"node\"><title>19</title>\r\n",
       "<path fill=\"#399de5\" stroke=\"black\" d=\"M1022.5,-179.5C1022.5,-179.5 934.5,-179.5 934.5,-179.5 928.5,-179.5 922.5,-173.5 922.5,-167.5 922.5,-167.5 922.5,-123.5 922.5,-123.5 922.5,-117.5 928.5,-111.5 934.5,-111.5 934.5,-111.5 1022.5,-111.5 1022.5,-111.5 1028.5,-111.5 1034.5,-117.5 1034.5,-123.5 1034.5,-123.5 1034.5,-167.5 1034.5,-167.5 1034.5,-173.5 1028.5,-179.5 1022.5,-179.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"978.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"978.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\r\n",
       "<text text-anchor=\"middle\" x=\"978.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 6]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"978.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = BrEven</text>\r\n",
       "</g>\r\n",
       "<!-- 18&#45;&gt;19 -->\r\n",
       "<g id=\"edge19\" class=\"edge\"><title>18&#45;&gt;19</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M978.5,-222.907C978.5,-212.204 978.5,-200.615 978.5,-189.776\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"982,-189.667 978.5,-179.667 975,-189.667 982,-189.667\"/>\r\n",
       "</g>\r\n",
       "<!-- 20 -->\r\n",
       "<g id=\"node21\" class=\"node\"><title>20</title>\r\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M1138,-179.5C1138,-179.5 1065,-179.5 1065,-179.5 1059,-179.5 1053,-173.5 1053,-167.5 1053,-167.5 1053,-123.5 1053,-123.5 1053,-117.5 1059,-111.5 1065,-111.5 1065,-111.5 1138,-111.5 1138,-111.5 1144,-111.5 1150,-117.5 1150,-123.5 1150,-123.5 1150,-167.5 1150,-167.5 1150,-173.5 1144,-179.5 1138,-179.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1101.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1101.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1101.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [4, 0]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"1101.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Loss</text>\r\n",
       "</g>\r\n",
       "<!-- 18&#45;&gt;20 -->\r\n",
       "<g id=\"edge20\" class=\"edge\"><title>18&#45;&gt;20</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1021.17,-222.907C1033.47,-211.211 1046.88,-198.457 1059.15,-186.78\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1061.8,-189.095 1066.63,-179.667 1056.97,-184.023 1061.8,-189.095\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x2c9104a96a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# Create decision tree object\n",
    "dot_data = export_graphviz(tree, filled=True, rounded=True,\n",
    "                           class_names=['Loss', 'BrEven'],\n",
    "                           feature_names=X.columns.values, \n",
    "                           out_file=None)\n",
    "\n",
    "# Visualize/Plot graph\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above decision tree plot helps us understand how the algorithm decides on a class. The coloring indicates the label. What we learn is that the driving factor for the given training set is the shelve location and after that the product's selling price. If shelve location is 0 (bad) we end up in the left side of the tree. Is the shelve location medium or good we run through the right hand side. `Gini` represents the resulting Gini index at that node, `samples` is the number of observations that end up in the respective node, `values` indicates the true label, e.g. 2nd node from the bottom-left `[4, 40]` means of the 44 samples 4 are of label `Loss` and 40 belong to `BrEven`. Following the majority vote, the decision tree algorithm labels all 44 samples in this node to class 'BrEven'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to save the decision tree to a `.pdf` or `.png` file, make sure to have the package `pydotplus` installed. You can use the same code snippet from above: \n",
    "* With `!pip list` check if the package is already installed\n",
    "* If not, use `!pip install pydotplus` to install the package\n",
    "\n",
    "Then you are ready to save the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pydotplus import graph_from_dot_data\n",
    "\n",
    "# Convert graph to dot-file\n",
    "graph = graph_from_dot_data(dot_data)\n",
    "\n",
    "# Here I safe 'graph' to the 'Graphics' folder as png\n",
    "# If pdf is prefered, replace both '...png' with '...pdf'\n",
    "graph.write_png('Graphics/0210_DT_Carseats.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees and Cross Validation\n",
    "\n",
    "We have mentioned above that one of the major disadvantages of decision trees is the risk of overfitting. Having discussed cross validation in the previous chapter, we ought to ask the question how cross validation can be used to prevent overfitting. The answer to this seemingly trivial question might be more complex than initially thought. If we apply a $k$-fold cross validation and generate $k$ decision tree for every fold in the training set, then these $k$ tree will most certainly look different from fold to fold. Each of the $k$ decision trees will still suffer from overfitting. Furthermore, CV does not tell us anything about which of the $k$ tree we are supposed to select for prediction, and ultimately, this ought to be our goal. If CV produces 10 different trees but doesn't tell us which one of these to choose, we have gained nothing. Choosing the one tree (among the $k$) with (for example) the lowest error rate will not do it as this approach is most probably flawed: such a model is based on even less information than what would be available through the full training set and in general models with more information beat those with less. So CV will not help us on this end. \n",
    "\n",
    "However, CV is still of use. To recapitulate the idea behind CV: Fundamentally, the purpose of cross validation is not to help select a particular instance of the $k$ decision trees but rather to qualify the model, i.e. to provide metrics such as error rate etc. which in turn can be useful in asserting the level of precision one can expect from the model. Therefore, CV comes into play when we are tuning the model to find the optimal hyperparameter. As an example: usually we do not know the optimal tree form. Does it generalize best when it has depth 2, 5, 10? Or is a stopping criterion of no more than 5, 10, 20 observation per region $R_j$ best? Here we can run different parameter in combination with CV and this, thus, will provide an answer on how well the model will generalize to new data - given the set of hyperparameter. \n",
    "\n",
    "#### Cross Validation\n",
    "Below we show two setups to do this: with loops or grid search. The first approach follows what we learned in the previous chapter on cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Max depth\n",
    "maxDepth = np.array([1, 2, 5, 10])\n",
    "\n",
    "# Minimum number of samples required to split any internal node \n",
    "minSamplesNode = np.array([2, 5, 10, 20])\n",
    "\n",
    "# The minimum number of samples required to be at a leaf/terminal node\n",
    "minSamplesLeaf = np.array([2, 5, 10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (depth =  1):  0.900 +/-  0.011\n",
      "Score (depth =  2):  0.866 +/-  0.050\n",
      "Score (depth =  5):  0.857 +/-  0.043\n",
      "Score (depth = 10):  0.851 +/-  0.070\n",
      "--------------------------------------------------\n",
      "Score (min sample at node =  2):  0.851 +/-  0.070\n",
      "Score (min sample at node =  5):  0.851 +/-  0.080\n",
      "Score (min sample at node = 10):  0.863 +/-  0.065\n",
      "Score (min sample at node = 20):  0.850 +/-  0.047\n",
      "--------------------------------------------------\n",
      "Score (min sample at leaf =  2):  0.854 +/-  0.080\n",
      "Score (min sample at leaf =  5):  0.866 +/-  0.050\n",
      "Score (min sample at leaf = 10):  0.869 +/-  0.047\n",
      "Score (min sample at leaf = 20):  0.869 +/-  0.056\n"
     ]
    }
   ],
   "source": [
    "# Import necessary functions\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# Create k-Fold CV object\n",
    "kFold = StratifiedKFold(n_splits=10, random_state=10)\n",
    "\n",
    "# Loop through maxDept values, run CV and print results\n",
    "for i in maxDepth:\n",
    "    tree = DecisionTreeClassifier(max_depth=i, random_state=0)\n",
    "    scrs = cross_val_score(tree, X_train, y_train, cv=kFold)\n",
    "    print('Score (depth ={0: 3.0f}): {1: .3f} +/- {2: .3f}'.format(i, np.mean(scrs), np.std(scrs)))\n",
    "\n",
    "print(50*'-')\n",
    "\n",
    "# Loop through minSamplesNode values, run CV and print results\n",
    "for i in minSamplesNode:\n",
    "    tree = DecisionTreeClassifier(min_samples_split=i, random_state=0)\n",
    "    scrs = cross_val_score(tree, X_train, y_train, cv=kFold)\n",
    "    print('Score (min sample at node ={0: 3.0f}): {1: .3f} +/- {2: .3f}'.format(i, np.mean(scrs), np.std(scrs)))\n",
    "    \n",
    "print(50*'-')\n",
    "\n",
    "# Loop through minSamplesNode values, run CV and print results\n",
    "for i in minSamplesLeaf:\n",
    "    tree = DecisionTreeClassifier(min_samples_leaf=i, random_state=0)\n",
    "    scrs = cross_val_score(tree, X_train, y_train, cv=kFold)\n",
    "    print('Score (min sample at leaf ={0: 3.0f}): {1: .3f} +/- {2: .3f}'.format(i, np.mean(scrs), np.std(scrs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output we can conclude that we get better scores with fewer nodes. As for the min sample at a node/leaf, the differences are too small to judge. However, what we can also conclude is that this is a fairly cumbersome process. Three separate loops to find the optimal hyperparameter. Furthermore, these three loops just check for one criterion, but what if we were interested in **all possible combinations**? Maybe we get better results if we combine them - we only know if we check. And, as you should be expecting by now, there's a convenient way of doing this: via grid search. \n",
    "\n",
    "#### Grid Search\n",
    "The approach of grid search is fairly simple: it's a brute-force search paradigm where we specify a list of values for different hyperparameters. The algorithm evaluates the model performance for each combination of hyperparameter to obtain the optimal combination of values from this set (Raschka (2015)). As usual we use a code example to show how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "{'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 2, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = {'criterion': ['gini', 'entropy'],\n",
    "              'max_depth': maxDepth,\n",
    "              'min_samples_split': minSamplesNode,\n",
    "              'min_samples_leaf': minSamplesLeaf}\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0),\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=kFold, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the preceding code, we train and tune a `DecisionTreeClassifier` on the given parameter grid. For this we define a dictionary called `param_grid` and apply this to the `GridSearchCV`. Using the training data we obtain the score of the best-performing model via the `best_score_` attribute (here based on the accuracy measure) and the corresponding parameter via the `best_params_`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As it turns out, we are unable to increase the accuracy score by combining stopping criterions. The best result is indeed when we use a max. depth of 1. If a combination of the three criterions (or the entropy criterion) were to yield better results, it would mean that `min_samples_leaf` and/or `min_samples_split` would have values $\\geq$ than the minimum value of 2. \n",
    "\n",
    "It is important to note that this result should not deceive you to believe that overfitting with decision trees is a myth. Here we seem to come across a rare exception where a prune tree of depth 1 yields the best performance. In general, decision trees have much better training results on deeper grown trees. Hence the risk of overfitting.\n",
    "\n",
    "> Note that grid search might be a convenient and powerful way of tuning hyperparameter but because it is a brute-force approach it is computationally very expensive. Depending on the number of processors you run your script on and the task at hand this might take substantial time. If, for whatever reasons, this is not feasible, the `RandomizedSearchCV` class might be a feasible alternative. This class draws random parameter from sampling distributions with a specified budget. See [the documentation for more details](http://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to estimate the performance of these parameter on the independent test dataset, we can run these three lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9500\n"
     ]
    }
   ],
   "source": [
    "# Extract best parameter\n",
    "clf = gs.best_estimator_\n",
    "\n",
    "# Fit model given best parameter\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print out score on Test dataset\n",
    "print('Test accuracy: {0: .4f}'.format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Forest\n",
    "### Turning Weaknesses into Strengths\n",
    "\n",
    "\n",
    "CV is so valuable because it provides reliable information on how well a model generalizes. Recall that given a set of $n$ independent observations $Z_1, Z_2, \\ldots, Z_n$, each with variance $\\sigma^2$, the variance of the mean $\\bar{Z}$ of the observations is given by $\\sigma^2/n$ (see appendix of the script). This shows that if we average a set of (independent) performance metrics (e.g. classification error), we actually reduce the variance of this error. And in doing so, we increase the validity of said metric. \n",
    "\n",
    "If we now extend this idea to not only assessing performance metrics but also predicting outcomes, we enter the field of **ensemble models**. These models produce $k$ independent predictions (e.g. trees) and assign the class label based on a majority vote of the $k$ outcomes. In doing so we lower the variance without compromising on the low bias of decision trees. Therefore it is easy to see that ensemble methods have proven to be extremely valuable, especially in the field of decision trees. The most prominent ensemble algorithm with respect to decision trees is called 'Random Forest'.\n",
    "\n",
    "Let us first define the steps of a random forest model (Raschka (2015, p. 90)):\n",
    "\n",
    "1. Draw a random bootstrap sample of size $n$ (randomly choose $n$ samples from the training set with replacement)\n",
    "2. Grow a decision tree from the bootstrap sample. At each node:\n",
    "    1. Randomly select $m$ features without replacement (with $m < p$)\n",
    "    2. Split the node using the feature that provides the best split according to the objective function (e.g. by maximizing the information gain)\n",
    "3. Repeat steps 1. and 2. $B$ times \n",
    "4. Aggregate the $B$ predictions (of each tree) and assign the class label by majority vote\n",
    "\n",
    "Step two above might seem odd at first: Why would we restrict the model to only choose from a subset $m$ of features (instead of selecting from the complete set $p$)? This is best explained with James et al. (2013, p. 320):\n",
    "\n",
    "> \"Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, all of the bagged trees will look quite similar to each other. Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities. [...] Random forests overcome this problem by forcing each split to consider only a subset of the predictors. Therefore, on average $(p - m)/p$ of the splits will not even consider the strong predictor, and so other predictors will have more of a chance. We can think of this process as *decorrelating* the trees, thereby making the average of the resulting trees less variable and hence more reliable.\"\n",
    "\n",
    "On a side note: There exists a predecessor algorithm that works similar to random forests except that it sets $m=p$ in step 2.1 per default. Python has it implement as `BaggingClassifier()`. Since random forests improves on the problem of correlated features, it is today clearly the preferred approach. For this reason we will only discuss the random forest implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Random Forest's Hyperparameter\n",
    "\n",
    "\n",
    "Though the interpretability of a random forest does not meet the simplicity of a simple decision tree, a big advantage is that we do not have to worry that much about choosing good hyperparameter values. We have three primary values to set: \n",
    "* The size $n$ of the bootstrap (step 1)\n",
    "* The subset $m$ of possible features (step 2)\n",
    "* The number of iterations $B$ (step 3)\n",
    "\n",
    "Typically, the larger number of trees $B$, the better the performance of our random forest classifier. But this of course comes at the expense of (potentially significant) increased computational costs. Additionally, the marginal improvement decreases as the number of trees is increased, i.e. at a certain point the cost in computation time will outgrow the benefit in prediction accuracy from more trees. In the Scikit-learn implementation, this hyperparameter is steered through the `n_estimators` argument.\n",
    "\n",
    "The feature subset size ($m$) to consider at each node is typically set to $m = \\sqrt{p}$, that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors $p$. Scikit-learn uses the `max_feature` argument to control for it. \n",
    "\n",
    "Finally, via the size $n$ of the bootstrap we control the bias-variance trade-off. A large value for $n$ will decrease randomness and thus such a model is more likely to overfit. On the other hand, preventing overfitting by selecting smaller values come at the expense of the model predictive performance. And since predictive accuracy is what we are most interested in, the vast majority of random forest implementations, including the `RandomForestClassifier` implementation in Scikit-learn, have set the bootstrap sample size $n$ per default to the number of samples in the original training set. This provides a good bias-variance trade-off (Raschka (2015)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest in Scikit-Learn\n",
    "\n",
    "There is an easily accessible implementation of a random forest classifier in Scikit-learn that we can use. For [a description of the available hyperparameter please check again the function's documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). It is also left to the reader to investigate how CV and/or grid-search can improve the performance. For this the same steps as explained for the `DecisionTreeClasifier` function can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create classifier object and fit it to data\n",
    "forest = RandomForestClassifier(criterion='gini', random_state=0, n_jobs=-1)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9500\n"
     ]
    }
   ],
   "source": [
    "# Print test score \n",
    "print('Test accuracy: {0: .4f}'.format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Ressources\n",
    "\n",
    "\n",
    "In writing this notebook, many ressources were consulted. For internet ressources the links are provided within the textflow above and will therefore not be listed again. Beyond these links, the following ressources were consulted and are recommended as further reading on the discussed topics:\n",
    "\n",
    "* Friedman, Jerome, Trevor Hastie, and Robert Tibshirani, 2001, *The Elements of Statistical Learning* (Springer, New York, NY).\n",
    "* James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani, 2013, *An Introduction to Statistical Learning: With Applications in R* (Springer Science & Business Media, New York, NY).\n",
    "* Müller, Andreas C., and Sarah Guido, 2017, *Introduction to Machine Learning with Python* (O’Reilly Media, Sebastopol, CA).\n",
    "* Raschka, Sebastian, 2015, *Python Machine Learning* (Packt Publishing Ltd., Birmingham, UK)\n",
    "* Vanderplas, Jake, 2016, *Python Data Science Handbook* (O'Reilly Media, Sebastopol, CA)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
